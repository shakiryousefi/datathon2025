{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-06 09:21:51 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 04-06 09:21:59 [config.py:585] This model supports multiple tasks: {'score', 'reward', 'classify', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 04-06 09:21:59 [config.py:1519] Defaulting to use mp for distributed inference\n",
      "INFO 04-06 09:21:59 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 04-06 09:22:00 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='Qwen/Qwen2.5-72B', speculative_config=None, tokenizer='Qwen/Qwen2.5-72B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-72B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 04-06 09:22:00 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 04-06 09:22:00 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 10485760, 10, 'psm_d3821269'), local_subscribe_addr='ipc:///tmp/bc0bc1ab-37bf-4b86-a0e4-c38d1ce2581b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 04-06 09:22:01 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x1554100423c0>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=119920)\u001b[0;0m INFO 04-06 09:22:01 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c5aaaee5'), local_subscribe_addr='ipc:///tmp/0a8e6ef5-54a6-400b-b12c-8e09e68dff77', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 04-06 09:22:02 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x15541896a0f0>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=119945)\u001b[0;0m INFO 04-06 09:22:02 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1d2ce41d'), local_subscribe_addr='ipc:///tmp/62d4444a-b35d-4cd1-862f-d2047c39759f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 04-06 09:22:03 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x1554100da0f0>\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=119982)\u001b[0;0m INFO 04-06 09:22:03 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d6d3ec05'), local_subscribe_addr='ipc:///tmp/473c14f7-915e-42f1-bde6-89fcfcacfbc2', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 04-06 09:22:04 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x15541aa590a0>\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=120014)\u001b[0;0m INFO 04-06 09:22:04 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_14f1a5b1'), local_subscribe_addr='ipc:///tmp/41a4625b-b4ff-4024-b1b8-59ca44ba6beb', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=119945)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=119920)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=120014)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=119982)\u001b[0;0m INFO 04-06 09:22:05 [utils.py:931] Found nccl from library libnccl.so.2\n",
      "INFO 04-06 09:22:05 [utils.py:931] Found nccl from library libnccl.so.2\n",
      "INFO 04-06 09:22:05 [utils.py:931] Found nccl from library libnccl.so.2\n",
      "INFO 04-06 09:22:05 [utils.py:931] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=119945)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=119920)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=119982)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=120014)\u001b[0;0m INFO 04-06 09:22:05 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 04-06 09:22:05 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 04-06 09:22:05 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 04-06 09:22:05 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=119982)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=120014)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=119945)\u001b[0;0m WARNING 04-06 09:22:06 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=119920)\u001b[0;0m WARNING 04-06 09:22:06 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 04-06 09:22:06 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 04-06 09:22:06 [custom_all_reduce.py:137] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=119920)\u001b[0;0m INFO 04-06 09:22:06 [shm_broadcast.py:259] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_7cfc9def'), local_subscribe_addr='ipc:///tmp/9583b11d-237c-4efc-9288-d28d046da72a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=119945)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=119982)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=119920)\u001b[0;0m INFO 04-06 09:22:06 [parallel_state.py:954] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "INFO 04-06 09:22:06 [parallel_state.py:954] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2\n",
      "INFO 04-06 09:22:06 [parallel_state.py:954] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=119945)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=119982)\u001b[0;0m INFO 04-06 09:22:06 [cuda.py:220] Using Flash Attention backend on V1 engine.\n",
      "INFO 04-06 09:22:06 [cuda.py:220] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=119920)\u001b[0;0m INFO 04-06 09:22:06 [cuda.py:220] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=120014)\u001b[0;0m INFO 04-06 09:22:06 [parallel_state.py:954] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=120014)\u001b[0;0m INFO 04-06 09:22:06 [cuda.py:220] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=119982)\u001b[0;0m INFO 04-06 09:22:06 [gpu_model_runner.py:1174] Starting to load model Qwen/Qwen2.5-72B...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=119945)\u001b[0;0m INFO 04-06 09:22:06 [gpu_model_runner.py:1174] Starting to load model Qwen/Qwen2.5-72B...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=119920)\u001b[0;0m INFO 04-06 09:22:06 [gpu_model_runner.py:1174] Starting to load model Qwen/Qwen2.5-72B...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=120014)\u001b[0;0m INFO 04-06 09:22:06 [gpu_model_runner.py:1174] Starting to load model Qwen/Qwen2.5-72B...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=119982)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=119945)\u001b[0;0m WARNING 04-06 09:22:06 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 04-06 09:22:06 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=119920)\u001b[0;0m WARNING 04-06 09:22:06 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=120014)\u001b[0;0m WARNING 04-06 09:22:06 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=119945)\u001b[0;0m INFO 04-06 09:22:06 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=120014)\u001b[0;0m INFO 04-06 09:22:06 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=119920)\u001b[0;0m INFO 04-06 09:22:06 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=119982)\u001b[0;0m INFO 04-06 09:22:06 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a346f3b5ca9b46ce9b56530222308383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/37 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=119945)\u001b[0;0m INFO 04-06 09:22:19 [loader.py:447] Loading weights took 12.29 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=119982)\u001b[0;0m INFO 04-06 09:22:19 [loader.py:447] Loading weights took 12.12 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=119945)\u001b[0;0m INFO 04-06 09:22:19 [gpu_model_runner.py:1186] Model loading took 34.0070 GB and 13.189715 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=119982)\u001b[0;0m INFO 04-06 09:22:19 [gpu_model_runner.py:1186] Model loading took 34.0070 GB and 13.311991 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=119920)\u001b[0;0m INFO 04-06 09:22:20 [loader.py:447] Loading weights took 12.69 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=119920)\u001b[0;0m INFO 04-06 09:22:20 [gpu_model_runner.py:1186] Model loading took 34.0070 GB and 14.515650 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=120014)\u001b[0;0m INFO 04-06 09:22:21 [loader.py:447] Loading weights took 13.70 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=120014)\u001b[0;0m INFO 04-06 09:22:21 [gpu_model_runner.py:1186] Model loading took 34.0070 GB and 15.201918 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=119920)\u001b[0;0m INFO 04-06 09:22:37 [backends.py:415] Using cache directory: /home/rahul/.cache/vllm/torch_compile_cache/02a2fb30e3/rank_0_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=119920)\u001b[0;0m INFO 04-06 09:22:37 [backends.py:425] Dynamo bytecode transform time: 15.80 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=120014)\u001b[0;0m INFO 04-06 09:22:38 [backends.py:415] Using cache directory: /home/rahul/.cache/vllm/torch_compile_cache/02a2fb30e3/rank_3_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=120014)\u001b[0;0m INFO 04-06 09:22:38 [backends.py:425] Dynamo bytecode transform time: 16.07 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=119982)\u001b[0;0m INFO 04-06 09:22:38 [backends.py:415] Using cache directory: /home/rahul/.cache/vllm/torch_compile_cache/02a2fb30e3/rank_2_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=119982)\u001b[0;0m INFO 04-06 09:22:38 [backends.py:425] Dynamo bytecode transform time: 16.15 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=119945)\u001b[0;0m INFO 04-06 09:22:38 [backends.py:415] Using cache directory: /home/rahul/.cache/vllm/torch_compile_cache/02a2fb30e3/rank_1_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=119945)\u001b[0;0m INFO 04-06 09:22:38 [backends.py:425] Dynamo bytecode transform time: 16.34 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=119920)\u001b[0;0m INFO 04-06 09:22:39 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=120014)\u001b[0;0m INFO 04-06 09:22:40 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=119982)\u001b[0;0m INFO 04-06 09:22:40 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=119945)\u001b[0;0m INFO 04-06 09:22:40 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=119982)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=119920)\u001b[0;0m INFO 04-06 09:22:53 [monitor.py:33] torch.compile takes 16.15 s in total\n",
      "INFO 04-06 09:22:53 [monitor.py:33] torch.compile takes 15.80 s in total\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=120014)\u001b[0;0m INFO 04-06 09:22:53 [monitor.py:33] torch.compile takes 16.07 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=119945)\u001b[0;0m INFO 04-06 09:22:53 [monitor.py:33] torch.compile takes 16.34 s in total\n",
      "INFO 04-06 09:22:58 [kv_cache_utils.py:566] GPU KV cache size: 455,456 tokens\n",
      "INFO 04-06 09:22:58 [kv_cache_utils.py:569] Maximum concurrency for 16,384 tokens per request: 27.80x\n",
      "INFO 04-06 09:22:58 [kv_cache_utils.py:566] GPU KV cache size: 455,344 tokens\n",
      "INFO 04-06 09:22:58 [kv_cache_utils.py:569] Maximum concurrency for 16,384 tokens per request: 27.79x\n",
      "INFO 04-06 09:22:58 [kv_cache_utils.py:566] GPU KV cache size: 455,344 tokens\n",
      "INFO 04-06 09:22:58 [kv_cache_utils.py:569] Maximum concurrency for 16,384 tokens per request: 27.79x\n",
      "INFO 04-06 09:22:58 [kv_cache_utils.py:566] GPU KV cache size: 455,856 tokens\n",
      "INFO 04-06 09:22:58 [kv_cache_utils.py:569] Maximum concurrency for 16,384 tokens per request: 27.82x\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=119920)\u001b[0;0m INFO 04-06 09:23:37 [gpu_model_runner.py:1534] Graph capturing finished in 39 secs, took 2.62 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=119982)\u001b[0;0m INFO 04-06 09:23:37 [gpu_model_runner.py:1534] Graph capturing finished in 39 secs, took 2.62 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=120014)\u001b[0;0m INFO 04-06 09:23:37 [gpu_model_runner.py:1534] Graph capturing finished in 39 secs, took 2.62 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=119945)\u001b[0;0m INFO 04-06 09:23:37 [gpu_model_runner.py:1534] Graph capturing finished in 39 secs, took 2.62 GiB\n",
      "INFO 04-06 09:23:37 [core.py:151] init engine (profile, create kv cache, warmup model) took 75.91 seconds\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Create an LLM\n",
    "llm = LLM(\n",
    "    model=\"Qwen/Qwen2.5-72B\",\n",
    "    gpu_memory_utilization=0.95,\n",
    "    max_model_len=16384,\n",
    "    tensor_parallel_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "from my_parser import process_zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZIP_PATHS = [\n",
    "    \"../data/datathon_part1.zip\",\n",
    "    \"../data/datathon_part2.zip\",\n",
    "    \"../data/datathon_part3.zip\",\n",
    "    \"../data/datathon_part4.zip\",\n",
    "]\n",
    "all_clients = process_zips(ZIP_PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_keys_to_none(d):\n",
    "\n",
    "    for key in d.keys():\n",
    "        if isinstance(d[key], dict):\n",
    "            set_keys_to_none(d[key])\n",
    "        else:\n",
    "            d[key] = \"\"\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'name': '', 'address': {'city': '', 'street name': '', 'street number': '', 'postal code': ''}, 'country_of_domicile': '', 'birth_date': '', 'nationality': '', 'passport_number': '', 'passport_issue_date': '', 'passport_expiry_date': '', 'gender': '', 'phone_number': '', 'email_address': '', 'marital_status': '', 'secondary_school': {'name': '', 'graduation_year': ''}, 'higher_education': '', 'employment_history': '', 'aum': {'savings': '', 'inheritance': '', 'real_estate_value': ''}, 'inheritance_details': {'relationship': '', 'inheritance year': '', 'profession': ''}, 'real_estate_details': '', 'investment_risk_profile': '', 'investment_horizon': '', 'investment_experience': '', 'type_of_mandate': '', 'preferred_markets': '', 'currency': ''}\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = all_clients[4][\"client_profile\"].copy()\n",
    "tmp = set_keys_to_none(tmp)\n",
    "\n",
    "field_string = str(tmp)\n",
    "field_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_prompt = f\"Does the description contain information that contradicts the client profile? If so, start your answer with YES, otherwise start your answer with NO.\"\n",
    "conflict_prompt = f\"Is there an obvious conflict of interest between the client and the bank? If so, start your answer with YES, otherwise start your answer with NO.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "false_positives = pkl.load(open(\"../false_positives.pkl\", \"rb\"))[:400]\n",
    "true_positives = pkl.load(open(\"../true_positives.pkl\", \"rb\"))[:400]\n",
    "\n",
    "considered_indices = false_positives + true_positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "\n",
    "def get_client_description_prompt():\n",
    "    return [\n",
    "        (\n",
    "            f\"Here is the client profile {all_clients[i]['client_profile']}\\n\"\n",
    "            f\"Here is the description {all_clients[i]['client_description']}\\n\"\n",
    "            \"Does the description contradict the client profile net worth?\\n\"\n",
    "            \"If this is the case say YES, otherwise say NO.\"\n",
    "        )\n",
    "        for i in considered_indices\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_age_prompt():\n",
    "    birth_dates = [\n",
    "        dt.datetime.strptime(c[\"client_profile\"][\"birth_date\"], \"%Y-%m-%d\")\n",
    "        for c in all_clients\n",
    "    ]\n",
    "    ages = [(dt.datetime.now() - birth_date).days // 365 for birth_date in birth_dates]\n",
    "\n",
    "    return [\n",
    "        (\n",
    "            f\"Here is the description {all_clients[i]['client_description']}\\n\"\n",
    "            f\"Is it true that the client is either {ages[i]-1} or {ages[i]} years old?\"\n",
    "            \"If the text does not mention the age, say IDK.\"\n",
    "            \"If the text has the same age, say YES. If it does not have the same age, say NO.\"\n",
    "            \"Justify why after the answer.\"\n",
    "        )\n",
    "        for i in considered_indices\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_marry_status_prompt():\n",
    "    return [\n",
    "        (\n",
    "            f\"Here is the client's marital status: <{all_clients[i]['client_profile']['marital_status']}>\\n\"\n",
    "            f\"Here is the Family Background of the client: <{all_clients[i]['client_description']['Family Background']}>\\n\"\n",
    "            \"Does the marital status match?\\n\"\n",
    "            \"If this is the case say YES, otherwise say NO.\"\n",
    "        )\n",
    "        for i in considered_indices\n",
    "    ]\n",
    "\n",
    "def get_education_prompt():\n",
    "    return [\n",
    "        (\n",
    "            f\"Here is the client's secondary school: <{all_clients[i]['client_profile']['secondary_school']}>\\n\"\n",
    "            f\"Here is the Education Background of the client: <{all_clients[i]['client_description']['Education Background']}>\\n\"\n",
    "            \"Does the secondary school match?\\n\"\n",
    "            \"If this is the case say YES, otherwise say NO.\"\n",
    "        )\n",
    "        for i in considered_indices\n",
    "    ]\n",
    "\n",
    "def get_higher_education_prompt():\n",
    "    return [\n",
    "        (\n",
    "            f\"Here is the client's higher education: {all_clients[i]['client_profile']['higher_education']}\\n\"\n",
    "            f\"Here is the Education Background of the client: {all_clients[i]['client_description']['Education Background']}\\n\"\n",
    "            \"Does the higher education match?\\n\"\n",
    "            \"If this is the case say YES, otherwise say NO.\"\n",
    "        )\n",
    "        for i in considered_indices\n",
    "    ]\n",
    "\n",
    "def get_inheritence_prompt():\n",
    "    return [\n",
    "        (\n",
    "            f\"Here is the inheritence of the client: <{all_clients[i]['client_profile']['aum']['inheritance']}>\\n\"\n",
    "            #f\"Here are the inheritence details of the client: <{all_clients[i]['client_profile']['inheritance_details']}>\\n\"\n",
    "            f\"Here is the Wealth Summary of the client: <{all_clients[i]['client_description']['Wealth Summary']}>\\n\"\n",
    "            \"Does the inheritence match?\\n\"\n",
    "            \"If this is the case say YES, otherwise say NO.\"\n",
    "        )\n",
    "        for i in considered_indices\n",
    "    ]\n",
    "\n",
    "def get_real_estate_value_prompt():\n",
    "    return [\n",
    "        (\n",
    "            f\"Here is the real-estate value of the client: <{all_clients[i]['client_profile']['aum']['real_estate_value']}>\\n\"\n",
    "            f\"Here is the Wealth Summary of the client: <{all_clients[i]['client_description']['Wealth Summary']}>\\n\"\n",
    "            \"Does the real-estate value match?\\n\"\n",
    "            \"If this is the case say YES, otherwise say NO.\"\n",
    "        )\n",
    "        for i in considered_indices\n",
    "    ]\n",
    "\n",
    "prompts = get_inheritence_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the inheritence of the client: <1620000>\n",
      "Here is the Wealth Summary of the client: <While working, she saved 340000 EUR, which she used to build a diversified investment portfolio.\n",
      "She owns house in Turin, where she resides. It is worth 1540000 EUR.\n",
      "\n",
      "In addition to her primary residence, she owns villa in Siena worth 1740000 EUR.\n",
      "\n",
      "She inherited a significant sum of 1620000 EUR from her grandmother, a respected Venture Capitalist, in 2006, which she has wisely invested.\n",
      ">\n",
      "Does the inheritence match?\n",
      "If this is the case say YES, otherwise say NO.\n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 800/800 [00:04<00:00, 198.15it/s, est. speed input: 27863.70 toks/s, output: 396.32 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# adjust sampling parameters as necessary for the task\n",
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=100, min_p=0.15, top_p=0.7)\n",
    "\n",
    "# Generate texts from the prompts\n",
    "outputs = llm.generate(prompts, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = [o.outputs[0].text for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_pos_rej = [\n",
    "    (i, a)\n",
    "    for i, a in zip(considered_indices, answer)\n",
    "    if a.startswith(\" YES\") and i in false_positives\n",
    "]\n",
    "\n",
    "len(false_pos_rej)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, ' YES'),\n",
       " (151, ' YES'),\n",
       " (183, ' YES'),\n",
       " (349, ' YES'),\n",
       " (370, ' YES'),\n",
       " (578, ' YES'),\n",
       " (690, ' YES'),\n",
       " (713, ' YES'),\n",
       " (758, ' YES'),\n",
       " (775, ' YES'),\n",
       " (932, ' YES'),\n",
       " (957, ' YES'),\n",
       " (976, ' YES'),\n",
       " (990, ' YES'),\n",
       " (1060, ' YES'),\n",
       " (1120, ' YES'),\n",
       " (1253, ' YES'),\n",
       " (1280, ' YES'),\n",
       " (1471, ' YES'),\n",
       " (1491, ' YES'),\n",
       " (1585, ' YES'),\n",
       " (1686, ' YES'),\n",
       " (1970, ' YES'),\n",
       " (2036, ' YES'),\n",
       " (2119, ' YES'),\n",
       " (2191, ' YES'),\n",
       " (2237, ' YES'),\n",
       " (2259, ' YES'),\n",
       " (2310, ' YES'),\n",
       " (2362, ' YES'),\n",
       " (2388, ' YES'),\n",
       " (2563, ' YES'),\n",
       " (2657, ' YES'),\n",
       " (2707, ' YES'),\n",
       " (2751, ' YES'),\n",
       " (2800, ' YES'),\n",
       " (2863, ' YES'),\n",
       " (2865, ' YES'),\n",
       " (2966, ' YES'),\n",
       " (3012, ' YES'),\n",
       " (3056, ' YES'),\n",
       " (3202, ' YES'),\n",
       " (3222, ' YES'),\n",
       " (3245, ' YES'),\n",
       " (3400, ' YES')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_pos_rej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_pos_rej = [\n",
    "    (i, a)\n",
    "    for i, a in zip(considered_indices, answer)\n",
    "    if a.startswith(\" YES\") and i in true_positives\n",
    "]\n",
    "\n",
    "len(true_pos_rej)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, ' YES'),\n",
       " (65, ' YES'),\n",
       " (67, ' YES'),\n",
       " (76, ' YES'),\n",
       " (82, ' YES'),\n",
       " (85, ' YES'),\n",
       " (88, ' YES'),\n",
       " (96, ' YES'),\n",
       " (98, ' YES'),\n",
       " (119, ' YES'),\n",
       " (121, ' YES'),\n",
       " (131, ' YES'),\n",
       " (168, ' YES'),\n",
       " (179, ' YES'),\n",
       " (186, ' YES'),\n",
       " (223, ' YES'),\n",
       " (253, ' YES'),\n",
       " (279, ' YES'),\n",
       " (282, ' YES'),\n",
       " (284, ' YES'),\n",
       " (300, ' YES'),\n",
       " (320, ' YES'),\n",
       " (328, ' YES'),\n",
       " (331, ' YES'),\n",
       " (335, ' YES'),\n",
       " (371, ' YES'),\n",
       " (375, ' YES'),\n",
       " (376, ' YES'),\n",
       " (383, ' YES'),\n",
       " (400, ' YES'),\n",
       " (415, ' YES'),\n",
       " (420, ' YES'),\n",
       " (449, ' YES'),\n",
       " (452, ' YES'),\n",
       " (516, ' YES'),\n",
       " (529, ' YES'),\n",
       " (543, ' YES'),\n",
       " (576, ' YES'),\n",
       " (614, ' YES'),\n",
       " (625, ' YES'),\n",
       " (637, ' YES'),\n",
       " (647, ' YES'),\n",
       " (680, ' YES'),\n",
       " (716, ' YES'),\n",
       " (737, ' YES'),\n",
       " (762, ' YES'),\n",
       " (769, ' YES'),\n",
       " (781, ' YES'),\n",
       " (787, ' YES'),\n",
       " (792, ' YES'),\n",
       " (795, ' YES'),\n",
       " (802, ' YES'),\n",
       " (806, ' YES'),\n",
       " (831, ' YES')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_pos_rej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passport': {'first_name': 'Julia',\n",
       "  'middle_name': 'Fleur',\n",
       "  'last_name': 'Schipper',\n",
       "  'gender': 'F',\n",
       "  'country': 'Netherlands',\n",
       "  'country_code': 'NLD',\n",
       "  'nationality': 'Dutch',\n",
       "  'birth_date': '1971-05-21',\n",
       "  'passport_number': 'BF3878404',\n",
       "  'passport_mrz': ['P<NLDSCHIPPER<<JULIA<FLEUR<<<<<<<<<<<<<<<<<<<',\n",
       "   'BF3878404NLD710521<<<<<<<<<<<<<<<<<<<<<<<<<<<'],\n",
       "  'passport_issue_date': '2023-07-04',\n",
       "  'passport_expiry_date': '2033-07-03'},\n",
       " 'client_profile': {'name': 'Julia Fleur Schipper',\n",
       "  'address': {'city': 'Haarlem',\n",
       "   'street name': 'Maliebaan',\n",
       "   'street number': 84,\n",
       "   'postal code': '1203 12'},\n",
       "  'country_of_domicile': 'Netherlands',\n",
       "  'birth_date': '1971-05-21',\n",
       "  'nationality': 'Dutch',\n",
       "  'passport_number': 'BF3878404',\n",
       "  'passport_issue_date': '2023-07-04',\n",
       "  'passport_expiry_date': '2033-07-03',\n",
       "  'gender': 'F',\n",
       "  'phone_number': '+31 06 93346627',\n",
       "  'email_address': 'julia.schipper@icloud.com',\n",
       "  'marital_status': 'widowed',\n",
       "  'secondary_school': {'name': 'Openbaar Lyceum Hengelo',\n",
       "   'graduation_year': 1991},\n",
       "  'higher_education': [{'university': 'Rotterdam University of Applied Sciences',\n",
       "    'graduation_year': 1994}],\n",
       "  'employment_history': [{'start_year': 1995,\n",
       "    'end_year': 1999,\n",
       "    'company': 'MSD Animal Health',\n",
       "    'position': 'Research Scientist',\n",
       "    'salary': 47000},\n",
       "   {'start_year': 2000,\n",
       "    'end_year': 2001,\n",
       "    'company': 'Novartis Farma B.V.',\n",
       "    'position': 'Clinical Trials Manager',\n",
       "    'salary': 59000},\n",
       "   {'start_year': 2001,\n",
       "    'end_year': None,\n",
       "    'company': 'Pfizer Nederland B.V.',\n",
       "    'position': 'Regulatory Affairs Director',\n",
       "    'salary': 474000}],\n",
       "  'aum': {'savings': 830000,\n",
       "   'inheritance': 420000,\n",
       "   'real_estate_value': 1540000},\n",
       "  'inheritance_details': {'relationship': 'grandmother',\n",
       "   'inheritance year': 2008,\n",
       "   'profession': 'Real Estate Developer'},\n",
       "  'real_estate_details': [{'property type': 'townhouse',\n",
       "    'property value': 1540000,\n",
       "    'property location': 'Haarlem'}],\n",
       "  'investment_risk_profile': 'Moderate',\n",
       "  'investment_horizon': 'Short',\n",
       "  'investment_experience': 'Experienced',\n",
       "  'type_of_mandate': 'Advisory',\n",
       "  'preferred_markets': ['France', 'Netherlands'],\n",
       "  'currency': 'EUR'},\n",
       " 'account_form': {'name': 'Julia Fleur Schipper',\n",
       "  'first_name': 'Julia',\n",
       "  'middle_name': 'Fleur',\n",
       "  'last_name': 'Schipper',\n",
       "  'passport_number': 'BF3878404',\n",
       "  'currency': 'EUR',\n",
       "  'address': {'city': 'Haarlem',\n",
       "   'street name': 'Maliebaan',\n",
       "   'street number': 84,\n",
       "   'postal code': '1203 12'},\n",
       "  'country_of_domicile': 'Netherlands',\n",
       "  'phone_number': '+31 06 93346627',\n",
       "  'email_address': 'julia.schipper@icloud.com'},\n",
       " 'client_description': {'Summary Note': \"Julia Fleur Schipper and the RM were introduced at a charity run in Amsterdam. They ran side by side and discussed their philanthropic goals, which led to a meaningful collaboration.\\nJulia Fleur Schipper is a 53 year old and comes from Netherlands.\\nShe was drawn to Julius Baer's holistic financial planning and investment strategies, prompting the decision to join.\\n\",\n",
       "  'Family Background': 'Julia Fleur Schipper is currently widowed. She has 3 kids called Van der Meer, Dekkers and Wagemakers.',\n",
       "  'Education Background': 'Julia finished secondary school at Openbaar Lyceum Hengelo in 1991.\\nShe then attended Rotterdam University of Applied Sciences, which she graduated in 1994.\\n',\n",
       "  'Occupation History': 'Julia Fleur Schipper is a 53 year old and comes from Netherlands.\\nAt MSD Animal Health, Julia Fleur Schipper held the position of Research Scientist from 1995 to 1999, developing essential skills in the field.\\nAt Novartis Farma B.V., she held the position of Clinical Trials Manager from 2000 to 2001, making significant contributions to the organization.\\nAt Pfizer Nederland B.V., she worked as a Regulatory Affairs Director from 2001 till now, focusing on strategic initiatives and team collaboration.\\n',\n",
       "  'Wealth Summary': 'While working, she saved 830000 EUR, which she used to build a diversified investment portfolio.\\nShe has townhouse located in Haarlem, worth 1540000 EUR.\\n\\nShe inherited a significant sum of 420000 EUR from her grandmother, a respected Real Estate Developer, in 2008, which she has wisely invested.\\n',\n",
       "  'Client Summary': 'The RM knows the client very well and is a good friend of the entire family.\\n'},\n",
       " 'label': {'label': 'Accept'}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_clients[61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
